# -*- coding: utf-8 -*-
"""Assignment6_JayaaEmekar24.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15esK8L_AouAwIRQRjJhmV_pl9yTIWMPo

Consider Figure 3 in node2vec: Scalable Feature Learning for Networks. Our goal for this
problem is to (roughly) reproduce this figure.

Implementation of Node2Vec from given repository
"""

import networkx as nx
from scipy import stats
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from collections import deque as que
import copy
from numpy.random import randint
import random
import math
import operator

from functools import reduce
from itertools import combinations_with_replacement

import numpy as np
import pkg_resources
from gensim.models import KeyedVectors
from tqdm import tqdm
from sklearn.cluster import KMeans
import os
import random
from collections import defaultdict
from abc import ABC, abstractmethod
import gensim
import pkg_resources
from joblib import Parallel, delayed
from tqdm.auto import tqdm

#from .parallel import parallel_generate_walks

def parallel_generate_walks(d_graph: dict, global_walk_length: int, num_walks: int, cpu_num: int,
                            sampling_strategy: dict = None, num_walks_key: str = None, walk_length_key: str = None,
                            neighbors_key: str = None, probabilities_key: str = None, first_travel_key: str = None,
                            quiet: bool = False) -> list:
    """
    Generates the random walks which will be used as the skip-gram input.

    :return: List of walks. Each walk is a list of nodes.
    """

    walks = list()

    if not quiet:
        pbar = tqdm(total=num_walks, desc='Generating walks (CPU: {})'.format(cpu_num))

    for n_walk in range(num_walks):

        # Update progress bar
        if not quiet:
            pbar.update(1)

        # Shuffle the nodes
        shuffled_nodes = list(d_graph.keys())
        random.shuffle(shuffled_nodes)

        # Start a random walk from every node
        for source in shuffled_nodes:

            # Skip nodes with specific num_walks
            if source in sampling_strategy and \
                    num_walks_key in sampling_strategy[source] and \
                    sampling_strategy[source][num_walks_key] <= n_walk:
                continue

            # Start walk
            walk = [source]

            # Calculate walk length
            if source in sampling_strategy:
                walk_length = sampling_strategy[source].get(walk_length_key, global_walk_length)
            else:
                walk_length = global_walk_length

            # Perform walk
            while len(walk) < walk_length:

                walk_options = d_graph[walk[-1]].get(neighbors_key, None)

                # Skip dead end nodes
                if not walk_options:
                    break

                if len(walk) == 1:  # For the first step
                    probabilities = d_graph[walk[-1]][first_travel_key]
                    walk_to = random.choices(walk_options, weights=probabilities)[0]
                else:
                    probabilities = d_graph[walk[-1]][probabilities_key][walk[-2]]
                    walk_to = random.choices(walk_options, weights=probabilities)[0]

                walk.append(walk_to)

            walk = list(map(str, walk))  # Convert all to strings

            walks.append(walk)

    if not quiet:
        pbar.close()

    return walks

class Node2Vec:
    FIRST_TRAVEL_KEY = 'first_travel_key'
    PROBABILITIES_KEY = 'probabilities'
    NEIGHBORS_KEY = 'neighbors'
    WEIGHT_KEY = 'weight'
    NUM_WALKS_KEY = 'num_walks'
    WALK_LENGTH_KEY = 'walk_length'
    P_KEY = 'p'
    Q_KEY = 'q'

    def __init__(self, graph: nx.Graph, dimensions: int = 128, walk_length: int = 80, num_walks: int = 10, p: float = 1,
                 q: float = 1, weight_key: str = 'weight', workers: int = 1, sampling_strategy: dict = None,
                 quiet: bool = False, temp_folder: str = None, seed: int = None):
        """
        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.

        :param graph: Input graph
        :param dimensions: Embedding dimensions (default: 128)
        :param walk_length: Number of nodes in each walk (default: 80)
        :param num_walks: Number of walks per node (default: 10)
        :param p: Return hyper parameter (default: 1)
        :param q: Inout parameter (default: 1)
        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')
        :param workers: Number of workers for parallel execution (default: 1)
        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.
        :param seed: Seed for the random number generator.
        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization
        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder
        """

        self.graph = graph
        self.dimensions = dimensions
        self.walk_length = walk_length
        self.num_walks = num_walks
        self.p = p
        self.q = q
        self.weight_key = weight_key
        self.workers = workers
        self.quiet = quiet
        self.d_graph = defaultdict(dict)

        if sampling_strategy is None:
            self.sampling_strategy = {}
        else:
            self.sampling_strategy = sampling_strategy

        self.temp_folder, self.require = None, None
        if temp_folder:
            if not os.path.isdir(temp_folder):
                raise NotADirectoryError("temp_folder does not exist or is not a directory. ({})".format(temp_folder))

            self.temp_folder = temp_folder
            self.require = "sharedmem"

        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)

        self._precompute_probabilities()
        self.walks = self._generate_walks()

    def _precompute_probabilities(self):
        """
        Precomputes transition probabilities for each node.
        """

        d_graph = self.d_graph

        nodes_generator = self.graph.nodes() if self.quiet \
            else tqdm(self.graph.nodes(), desc='Computing transition probabilities')

        for source in nodes_generator:

            # Init probabilities dict for first travel
            if self.PROBABILITIES_KEY not in d_graph[source]:
                d_graph[source][self.PROBABILITIES_KEY] = dict()

            for current_node in self.graph.neighbors(source):

                # Init probabilities dict
                if self.PROBABILITIES_KEY not in d_graph[current_node]:
                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()

                unnormalized_weights = list()
                d_neighbors = list()

                # Calculate unnormalized weights
                for destination in self.graph.neighbors(current_node):

                    p = self.sampling_strategy[current_node].get(self.P_KEY,
                                                                 self.p) if current_node in self.sampling_strategy else self.p
                    q = self.sampling_strategy[current_node].get(self.Q_KEY,
                                                                 self.q) if current_node in self.sampling_strategy else self.q

                    if destination == source:  # Backwards probability
                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / p
                    elif destination in self.graph[source]:  # If the neighbor is connected to the source
                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1)
                    else:
                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / q

                    # Assign the unnormalized sampling strategy weight, normalize during random walk
                    unnormalized_weights.append(ss_weight)
                    d_neighbors.append(destination)

                # Normalize
                unnormalized_weights = np.array(unnormalized_weights)
                d_graph[current_node][self.PROBABILITIES_KEY][
                    source] = unnormalized_weights / unnormalized_weights.sum()

            # Calculate first_travel weights for source
            first_travel_weights = []

            for destination in self.graph.neighbors(source):
                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))

            first_travel_weights = np.array(first_travel_weights)
            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()

            # Save neighbors
            d_graph[source][self.NEIGHBORS_KEY] = list(self.graph.neighbors(source))

    def _generate_walks(self) -> list:
        """
        Generates the random walks which will be used as the skip-gram input.
        :return: List of walks. Each walk is a list of nodes.
        """

        flatten = lambda l: [item for sublist in l for item in sublist]

        # Split num_walks for each worker
        num_walks_lists = np.array_split(range(self.num_walks), self.workers)

        walk_results = Parallel(n_jobs=self.workers, temp_folder=self.temp_folder, require=self.require)(
            delayed(parallel_generate_walks)(self.d_graph,
                                             self.walk_length,
                                             len(num_walks),
                                             idx,
                                             self.sampling_strategy,
                                             self.NUM_WALKS_KEY,
                                             self.WALK_LENGTH_KEY,
                                             self.NEIGHBORS_KEY,
                                             self.PROBABILITIES_KEY,
                                             self.FIRST_TRAVEL_KEY,
                                             self.quiet) for
            idx, num_walks
            in enumerate(num_walks_lists, 1))

        walks = flatten(walk_results)

        return walks

    def fit(self, **skip_gram_params) -> gensim.models.Word2Vec:
        """
        Creates the embeddings using gensim's Word2Vec.
        :param skip_gram_params: Parameters for gensim.models.Word2Vec - do not supply 'size' / 'vector_size' it is
            taken from the Node2Vec 'dimensions' parameter
        :type skip_gram_params: dict
        :return: A gensim word2vec model
        """

        if 'workers' not in skip_gram_params:
            skip_gram_params['workers'] = self.workers

        # Figure out gensim version, naming of output dimensions changed from size to vector_size in v4.0.0
        gensim_version = pkg_resources.get_distribution("gensim").version
        size = 'size' if gensim_version < '4.0.0' else 'vector_size'
        if size not in skip_gram_params:
            skip_gram_params[size] = self.dimensions

        if 'sg' not in skip_gram_params:
            skip_gram_params['sg'] = 1

        return gensim.models.Word2Vec(self.walks, **skip_gram_params)

class EdgeEmbedder(ABC):
    INDEX_MAPPING_KEY = 'index2word' if pkg_resources.get_distribution("gensim").version < '4.0.0' else 'index_to_key'

    def __init__(self, keyed_vectors: KeyedVectors, quiet: bool = False):
        """
        :param keyed_vectors: KeyedVectors containing nodes and embeddings to calculate edges for
        """

        self.kv = keyed_vectors
        self.quiet = quiet

    @abstractmethod
    def _embed(self, edge: tuple) -> np.ndarray:
        """
        Abstract method for implementing the embedding method
        :param edge: tuple of two nodes
        :return: Edge embedding
        """
        pass

    def __getitem__(self, edge) -> np.ndarray:
        if not isinstance(edge, tuple) or not len(edge) == 2:
            raise ValueError('edge must be a tuple of two nodes')

        if edge[0] not in getattr(self.kv, self.INDEX_MAPPING_KEY):
            raise KeyError('node {} does not exist in given KeyedVectors'.format(edge[0]))

        if edge[1] not in getattr(self.kv, self.INDEX_MAPPING_KEY):
            raise KeyError('node {} does not exist in given KeyedVectors'.format(edge[1]))

        return self._embed(edge)

    def as_keyed_vectors(self) -> KeyedVectors:
        """
        Generated a KeyedVectors instance with all the possible edge embeddings
        :return: Edge embeddings
        """

        edge_generator = combinations_with_replacement(getattr(self.kv, self.INDEX_MAPPING_KEY), r=2)

        if not self.quiet:
            vocab_size = len(getattr(self.kv, self.INDEX_MAPPING_KEY))
            total_size = reduce(lambda x, y: x * y, range(1, vocab_size + 2)) / \
                         (2 * reduce(lambda x, y: x * y, range(1, vocab_size)))

            edge_generator = tqdm(edge_generator, desc='Generating edge features', total=total_size)

        # Generate features
        tokens = []
        features = []
        for edge in edge_generator:
            token = str(tuple(sorted(edge)))
            embedding = self._embed(edge)

            tokens.append(token)
            features.append(embedding)

        # Build KV instance
        edge_kv = KeyedVectors(vector_size=self.kv.vector_size)
        if pkg_resources.get_distribution("gensim").version < '4.0.0':
            edge_kv.add(
                entities=tokens,
                weights=features)
        else:
            edge_kv.add_vectors(
                keys=tokens,
                weights=features)

        return edge_kv


class AverageEmbedder(EdgeEmbedder):
    """
    Average node features
    """

    def _embed(self, edge: tuple):
        return (self.kv[edge[0]] + self.kv[edge[1]]) / 2


class HadamardEmbedder(EdgeEmbedder):
    """
    Hadamard product node features
    """

    def _embed(self, edge: tuple):
        return self.kv[edge[0]] * self.kv[edge[1]]


class WeightedL1Embedder(EdgeEmbedder):
    """
    Weighted L1 node features
    """

    def _embed(self, edge: tuple):
        return np.abs(self.kv[edge[0]] - self.kv[edge[1]])


class WeightedL2Embedder(EdgeEmbedder):
    """
    Weighted L2 node features
    """

    def _embed(self, edge: tuple):
        return (self.kv[edge[0]] - self.kv[edge[1]]) ** 2

"""• Download the Les Mis´erables coapperance network and read it into NetworkX (the read_gml function should be useful here).

• Use the parameters detailed in section 4.1 of node2vec: Scalable Feature Learning for Networks to construct two embeddings of this network.
"""

# FILES
EMBEDDING_FILENAME_COMM = './embeddings_comm.emb'
EMBEDDING_MODEL_FILENAME_COMM = './embeddings_comm.model'

EMBEDDING_FILENAME_STRUCT_EQ = './embeddings_struct_eq.emb'
EMBEDDING_MODEL_FILENAME_STRUCT_EQ = './embeddings_struct_eq.model'

graph = nx.read_gml('/content/lesmis.gml',label='id')
graph = nx.Graph(graph)
graph.remove_edges_from(nx.selfloop_edges(graph))

print(graph.number_of_edges())
print(graph.number_of_nodes())

# Communities
node2vec_comm = Node2Vec(graph, dimensions=16, walk_length=30, num_walks=100, workers=1, p=1, q=0.5)
model_comm = node2vec_comm.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)

#Structual Equivalence
node2vec_struct_eq = Node2Vec(graph, dimensions=16, walk_length=3, num_walks=100, workers=4, p=1, q=2)
model_struct_eq = node2vec_struct_eq.fit(window=3, min_count=1, batch_words=3)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)

# Save embeddings for later use
model_comm.wv.save_word2vec_format(EMBEDDING_FILENAME_COMM)

# Save model for later use
model_comm.save(EMBEDDING_MODEL_FILENAME_COMM)

# Save embeddings for later use
model_struct_eq.wv.save_word2vec_format(EMBEDDING_FILENAME_STRUCT_EQ)

# Save model for later use
model_struct_eq.save(EMBEDDING_MODEL_FILENAME_STRUCT_EQ)

"""• Apply k-means clustering with k = 6 on the embedding created with p = 1 and q = 0.5
(looking for communities). This should give you six groups of nodes with different labels.

• Finally, use NetworkX to generate two visualizations of this network with nodes in the same
cluster given the same color. Your visualizations do not need to look exactly like those in
the paper. You should, however, be able to pick out similar groups of nodes.
"""

nodes_comm = pd.read_csv('/content/embeddings_comm.emb', skiprows=range(0, 1), sep=' ', header=None)
#nodes_comm = pd.read_csv('/content/embeddings_struct_eq.emb', skiprows=range(0, 1), sep=' ', header=None)
nodes_comm = nodes_comm.sort_values(by=0).reset_index(drop=True)
cluster_comm = KMeans(n_clusters=6)
cluster_comm.fit(nodes_comm.iloc[:, 1:])
cluster_comm.labels_
pos_comm = nx.drawing.layout.spring_layout(graph)
#pos_comm=nx.draw(graph)
plt.figure(figsize=(18,10))
ax = plt.gca()
ax.set_title('Lookout for Communities graph')
d = dict(graph.degree)
nx.draw_networkx_edges(graph, pos=pos_comm, alpha=0.1)
nx.draw(graph, pos=pos_comm, with_labels=False, nodelist=sorted(graph.nodes()), node_color=cluster_comm.labels_, node_size=[v*24 for v in d.values()], ax=ax)

"""• Apply k-means clustering with k = 3 on the embedding with p = 1 and q = 2 (looking for
structural equivalence). This should give you three groups of nodes with different labels.

• Finally, use NetworkX to generate two visualizations of this network with nodes in the same
cluster given the same color. Your visualizations do not need to look exactly like those in
the paper. You should, however, be able to pick out similar groups of nodes.
"""

nodes_struct_eq = pd.read_csv('/content/embeddings_struct_eq.emb', skiprows=range(0, 1), sep=' ', header=None)
#nodes_struct_eq = pd.read_csv('/content/embeddings_comm.emb', skiprows=range(0, 1), sep=' ', header=None)
nodes_struct_eq = nodes_struct_eq.sort_values(by=0).reset_index(drop=True)
cluster_struct_eq = KMeans(n_clusters=3)
cluster_struct_eq.fit(nodes_struct_eq.iloc[:, 1:])
cluster_struct_eq.labels_
pos_struct_eq = nx.drawing.layout.spring_layout(graph)
plt.figure(figsize=(18,10))
ax = plt.gca()
ax.set_title('Structural Equivalence graph')
d = dict(graph.degree)
nx.draw_networkx_edges(graph, pos=[v for v in pos_struct_eq.values()], alpha=0.1)
nx.draw(graph, pos=pos_struct_eq, with_labels=False, nodelist=sorted(graph.nodes()), node_color=cluster_struct_eq.labels_, node_size=[v*24 for v in d.values()], ax=ax)



# -*- coding: utf-8 -*-
"""Assignment-6-JayaaEmekar24ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKVJTyL4xhNMjmPL2hSPoeRq7v1CMC0r

**Question 5**

Generate a figure showing the average harmonic centrality of nodes in Erd¨os-R´enyi random
graphs with 1000 nodes and p ranging from 0.05 to 0.95 in increments of 0.05. For each value of
p you only need to consider a single graph but take an average over all nodes in the graph. In
particular, values of p will be on the x-axis and average harmonic centralities should be on the
y-axis. You may use the harmonic_centrality function available in NetworkX

Answer
"""

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

"""Function to plot the mean hormanic centrality on Y axis vs Probablity(p) on X-axis"""

# plot the mean hormanic centrality on Y axis vs Probablity(p) on X-axis
def plot_graph_linear_log_axis(plot_data_x,plot_data_y,linearLog):
 # create plot on linear axis
 fig, ax = plt.subplots(figsize=(12, 6))
 ax.scatter(plot_data_x, plot_data_y)
 ax.set_title('Mean hormanic centrality Vs Probablity - Linear Scale')
 ax.set_xlabel('Probablity')
 ax.set_ylabel('Mean hormanic centrality')
 if linearLog:
  plt.yscale('log')
  plt.xscale('log')
  ax.set_title('Mean hormanic centrality Vs Probablity- Log Scale')
 plt.show()

"""Function to find avgrage hormonic centrality"""

#Function to find avgrage hormonic centrality
def find_avg_hormanic_centrality(G,plot_data_y):
 #calculate average neighbor degree
 avg_harmonic_centrality = nx.harmonic_centrality(G)
 mean_harmonic_centrality = 0
 n = len(G.nodes)
 for val in avg_harmonic_centrality:
  mean_harmonic_centrality += avg_harmonic_centrality[val]
  mean_harmonic_centrality /= n
 plot_data_y.append(mean_harmonic_centrality) 
 return plot_data_y

"""Find average harmonic centrality of nodes in Erd¨os-R´enyi random graphs with 1000 nodes and p ranging from 0.05 to 0.95 in increments of 0.05."""

#Find average harmonic centrality of nodes in Erd¨os-R´enyi random graphs 
#with 1000 nodes and p ranging from 0.05 to 0.95 in increments of 0.05.
plot_data_x = []
plot_data_y = []
for i in np.arange(0.05, 0.95, 0.05):
  plot_data_x.append(i)
  G= nx.erdos_renyi_graph(1000,i)
  plot_data_y = find_avg_hormanic_centrality(G,plot_data_y)

"""Plot the mean hormanic centrality on Y axis vs probablity(p) on X-axis with linear scale"""

plot_graph_linear_log_axis(plot_data_x,plot_data_y,False)

"""Plot the mean hormanic centrality on Y axis vs probablity(p) on X-axis with log scale"""

plot_graph_linear_log_axis(plot_data_x,plot_data_y,True)

""".

.
"""